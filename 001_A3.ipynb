{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Polaris0116/awesome_lists/blob/main/001_A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1xJR1RtTnuH"
      },
      "source": [
        "## 注意事项：\n",
        "## 1. 删除不必要的comment（注释）和Markdown（双击DD可删除）\n",
        "## 2. 修改variable（即：变量）名称和数据集的获取路径\n",
        "## 3. 修改模型的设定参数（filters_layer, kernel_size, strides, pool_size, activation等）\n",
        "## 3. 修改训练的设定参数（VALIDATION_SPLIT, NUM_TRAILS, EPOCH等）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_h9QkAYTnuH"
      },
      "source": [
        "## 本次作业要求的两个文件我已合并成为此一个文件了，最后有模型的总结和准确率显示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vuckii6TnuH"
      },
      "source": [
        "## 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y1txoJHTnuI"
      },
      "outputs": [],
      "source": [
        "# Besic Libraries\n",
        "import numpy as np\n",
        "\n",
        "# Neural Networks\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Data Augmentation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Early Stopping\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Import Hyper-parameters tuning utility\n",
        "import optuna\n",
        "\n",
        "# Save the best model\n",
        "from os import path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbHGbkQaTnuJ"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters for tranining process (Not the model params)\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 20\n",
        "VALIDATION_SPLIT = 0.2 # Validation during training\n",
        "NUM_CLASSES = 10 # Number of target\n",
        "NUM_TRAILS = 1 # Number of trails\n",
        "TIME_OUT = 2000  # Timeout is optinal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CudJmlvUTnuJ",
        "outputId": "d310d837-52a1-4d13-d8c0-4f1304e56ea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "\n",
            "The image dimension is as follows:\n",
            "28 28\n"
          ]
        }
      ],
      "source": [
        "# Load MNIST dataset\n",
        "# Load training and testing dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data(\"/Users/steven/mnist.npz\")\n",
        "\n",
        "# Check Shape\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "print(\"\")\n",
        "\n",
        "# Check demisions of the image\n",
        "# Image dimention: 28*28 pixels\n",
        "img_rows, img_cols = x_train.shape[1:]\n",
        "print(\"The image dimension is as follows:\")\n",
        "print(img_rows, img_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e452ZBpbTnuL",
        "outputId": "3853c5b6-3918-4168-fe45-0b5d817edf2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(28, 28, 1)\n"
          ]
        }
      ],
      "source": [
        "# 数据转换，进行数据标准化之后，能够更好利用模型进行训练\n",
        "if keras.backend.image_data_format() == 'channels_first':\n",
        "    # x_train.shape = (60000, 1, 28, 28)\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    # x_train.shape = (60000, 28, 28, 1)\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# Convert pixel values to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "# Print X_train Shape\n",
        "print(x_train.shape)\n",
        "# Print Input Shape\n",
        "print(input_shape)\n",
        "\n",
        "# Convert y to one-hot\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRLvuUSVTnuL"
      },
      "source": [
        "## 2. Model Params Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh9cGieNTnuM"
      },
      "outputs": [],
      "source": [
        "# Define an objective function to be maximized\n",
        "def objective(trial):\n",
        "    # Construct a sequential model\n",
        "    model = keras.models.Sequential()\n",
        "    # Suggest values of the model hyperparameters\n",
        "\n",
        "    filters_layer1 = trial.suggest_categorical(\"filters_layer1\", [96, 128, 512])\n",
        "    filters_layer2 = trial.suggest_categorical(\"filters_layer2\", [32, 64, 96])\n",
        "\n",
        "    kernel_size = trial.suggest_categorical(\"kernel_size\", [3, 4])\n",
        "    strides = trial.suggest_categorical(\"strides\", [1, 2, 3])\n",
        "    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n",
        "    pool_size = trial.suggest_categorical(\"pool_size\", [1, 2, 3])\n",
        "\n",
        "    # Add 1st Conv2D Layer\n",
        "    model.add(layers.Conv2D(filters=filters_layer1,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=strides,\n",
        "                            activation=activation,\n",
        "                            padding=\"same\",\n",
        "                            input_shape=input_shape))\n",
        "    # Add Pooling Layer\n",
        "    model.add(layers.MaxPool2D(pool_size=pool_size))\n",
        "    # Add 2nd Conv2D Layer\n",
        "    model.add(layers.Conv2D(filters=filters_layer2,\n",
        "                            kernel_size=kernel_size,\n",
        "                            strides=strides,\n",
        "                            activation=activation,\n",
        "                            padding=\"same\"))\n",
        "    # Add Pooling Layer\n",
        "    model.add(layers.MaxPool2D(pool_size=pool_size))\n",
        "    # Add Dropout Layer\n",
        "    model.add(layers.Dropout(rate=trial.suggest_categorical(\"rate\", [0.1, 0.15, 0.2])))\n",
        "    # Add Flatten Layer\n",
        "    model.add(layers.Flatten())\n",
        "    # Add  1st Fully-connected Layer\n",
        "    model.add(layers.Dense(units=trial.suggest_categorical(\"units\", [16, 32, 128]), activation=\"relu\"))\n",
        "    # Add  2nd Fully-connected Layer\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "    # Compile the model\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "    # Define early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor=\"loss\", patience=7)\n",
        "\n",
        "    # Train model with augmented data\n",
        "    history = model.fit(x_train, y_train, callbacks=[early_stopping], batch_size=BATCH_SIZE, epochs=EPOCH, verbose=1)\n",
        "\n",
        "    # Evaluate the model accuracy on the testing set.\n",
        "    score = model.evaluate(x_test, y_test, verbose=1) # 1 for showing progress bar\n",
        "    # Accuracy is the 1st element of the score list\n",
        "    return score[1] # Accuracy is the 1st element of the score list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "xIR_oiaWTnuN",
        "outputId": "b2f3de3d-f640-45dd-a311-792c7b99945c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-12-09 23:45:18,363] A new study created in memory with name: no-name-d80ae026-81c0-4420-b991-4b223153ee68\n",
            "2023-12-09 23:45:18.367362: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.4468 - accuracy: 0.8729\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 12s 27ms/step - loss: 0.1176 - accuracy: 0.9647\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0824 - accuracy: 0.9749\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0658 - accuracy: 0.9804\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0536 - accuracy: 0.9843\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0479 - accuracy: 0.9849\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0404 - accuracy: 0.9875\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0365 - accuracy: 0.9886\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0333 - accuracy: 0.9893\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 12s 27ms/step - loss: 0.0280 - accuracy: 0.9908\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 13s 29ms/step - loss: 0.0276 - accuracy: 0.9909\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.0227 - accuracy: 0.9927\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.0207 - accuracy: 0.9933\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0201 - accuracy: 0.9934\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0183 - accuracy: 0.9939\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 13s 28ms/step - loss: 0.0177 - accuracy: 0.9942\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0164 - accuracy: 0.9945\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0143 - accuracy: 0.9952\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0133 - accuracy: 0.9955\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 12s 27ms/step - loss: 0.0138 - accuracy: 0.9954\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-12-09 23:49:31,883] Trial 0 finished with value: 0.9891999959945679 and parameters: {'filters_layer1': 128, 'filters_layer2': 96, 'kernel_size': 3, 'strides': 2, 'activation': 'tanh', 'pool_size': 2, 'rate': 0.15, 'units': 16}. Best is trial 0 with value: 0.9891999959945679.\n"
          ]
        }
      ],
      "source": [
        "# Create a study object and optimize the objective function.\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=NUM_TRAILS) # you can even set time limit to end this trial: timeout=TIME_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "stjesrZ1TnuN",
        "outputId": "91c6a840-39d2-43de-e69b-99d7c695c4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 1\n",
            "\n",
            "Aaacuacy for the Best Trial: 0.9891999959945679\n",
            "\n",
            "Params for the Best Trial: \n",
            "filters_layer1: 128\n",
            "filters_layer2: 96\n",
            "kernel_size: 3\n",
            "strides: 2\n",
            "activation: tanh\n",
            "pool_size: 2\n",
            "rate: 0.15\n",
            "units: 16\n"
          ]
        }
      ],
      "source": [
        "# Print total numebr of trails\n",
        "print(f\"Number of finished trials: {len(study.trials)}\")\n",
        "print(\"\")\n",
        "# Find the best trial\n",
        "best_trial = study.best_trial\n",
        "print(f\"Aaacuacy for the Best Trial: {best_trial.value}\")\n",
        "print(\"\")\n",
        "print(\"Params for the Best Trial: \")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEGlN01uTnuO"
      },
      "source": [
        "## 3. Model Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i_YWqZJTnuO",
        "outputId": "25916771-f3df-41bc-e8e7-6a06cf3fc692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best model hyper-parameters are as follows:\n",
            "[128, 96, 3, 2, 'tanh', 2, 0.15, 16]\n"
          ]
        }
      ],
      "source": [
        "# Acquire Best Model Hyper-parameters\n",
        "\n",
        "best_params_list = []\n",
        "for key, value in best_trial.params.items():\n",
        "    best_params_list.append(value)\n",
        "print(\"The best model hyper-parameters are as follows:\")\n",
        "print(best_params_list)\n",
        "\n",
        "filters_layer1 = best_params_list[0]\n",
        "filters_layer2 = best_params_list[1]\n",
        "kernel_size = best_params_list[2]\n",
        "strides = best_params_list[3]\n",
        "pool_size = best_params_list[5]\n",
        "rate = best_params_list[6]\n",
        "unit = best_params_list[7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EdpG9UCTTnuO",
        "outputId": "314fa951-514f-4e1b-86d0-a215ddec4f70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model constructing ...\n",
            "Epoch 1/20\n",
            "469/469 [==============================] - 11s 23ms/step - loss: 0.4040 - accuracy: 0.8752\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1159 - accuracy: 0.9654\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0840 - accuracy: 0.9743\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0654 - accuracy: 0.9801\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0582 - accuracy: 0.9820\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0494 - accuracy: 0.9851\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0434 - accuracy: 0.9872\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0377 - accuracy: 0.9880\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0340 - accuracy: 0.9891\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.0309 - accuracy: 0.9901\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0260 - accuracy: 0.9916\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0254 - accuracy: 0.9914\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0214 - accuracy: 0.9933\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0204 - accuracy: 0.9932\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0174 - accuracy: 0.9943\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0182 - accuracy: 0.9936\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 12s 26ms/step - loss: 0.0155 - accuracy: 0.9947\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 13s 27ms/step - loss: 0.0147 - accuracy: 0.9949\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0119 - accuracy: 0.9961\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.0127 - accuracy: 0.9959\n"
          ]
        }
      ],
      "source": [
        "# Best CNN Model Reconstruction\n",
        "best_model = keras.models.Sequential()\n",
        "\n",
        "best_model.add(layers.Conv2D(filters=filters_layer1,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=strides,\n",
        "                        activation=\"relu\",\n",
        "                        padding=\"same\",\n",
        "                        input_shape=input_shape))\n",
        "\n",
        "best_model.add(layers.MaxPool2D(pool_size=pool_size))\n",
        "\n",
        "best_model.add(layers.Conv2D(filters=filters_layer2,\n",
        "                        kernel_size=kernel_size,\n",
        "                        strides=strides,\n",
        "                        activation=\"relu\",\n",
        "                        padding=\"same\"))\n",
        "# Add Pooling Layer\n",
        "best_model.add(layers.MaxPool2D(pool_size=pool_size))\n",
        "# Add Dropout Layer\n",
        "best_model.add(layers.Dropout(rate=rate))\n",
        "# Add Flatten Layer\n",
        "best_model.add(layers.Flatten())\n",
        "# Add  1st Fully-connected Layer\n",
        "best_model.add(layers.Dense(units=unit, activation=\"relu\"))\n",
        "# Add  2nd Fully-connected Layer\n",
        "best_model.add(layers.Dense(NUM_CLASSES, activation=\"softmax\"))\n",
        "\n",
        "# Compile the best model\n",
        "best_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor=\"loss\", patience=5)\n",
        "\n",
        "# Train the best model with augmented data\n",
        "print(\"Model constructing ...\")\n",
        "history = best_model.fit(x_train, y_train, callbacks=[early_stopping], batch_size=BATCH_SIZE, epochs=EPOCH, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6LgZigKTnuP"
      },
      "source": [
        "## Model Summary & Performance (ACC and LOSS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qV8n7-ohTnuP",
        "outputId": "788858eb-504a-4810-b3d1-0c39391b6465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 14, 14, 128)       1280      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 4, 4, 96)          110688    \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 2, 2, 96)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 2, 2, 96)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 384)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                6160      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 118,298\n",
            "Trainable params: 118,298\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Best Model Summary\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gazLvikWTnuP",
        "outputId": "7866d3b3-bd5c-4d2b-d831-2869d1f463b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0374 - accuracy: 0.9891\n",
            "\n",
            "Model performances are as follows:\n",
            "Loss: 0.03738785535097122\n",
            "Accuracy: 0.9890999794006348\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model accuracy on the testing set.\n",
        "score = best_model.evaluate(x_test, y_test, verbose=1) # 1 for showing progress bar\n",
        "# Accuracy is the 1st element of the score list\n",
        "print(\"\")\n",
        "print(\"Model performances are as follows:\")\n",
        "print(f\"Loss: {score[0]}\")\n",
        "print(f\"Accuracy: {score[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTepAmxZTnuP"
      },
      "source": [
        "## 4. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1jKvNOyTnuQ",
        "outputId": "433ab1a5-67b7-4552-df8e-99e7002f4fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best model has been saved successfully.\n"
          ]
        }
      ],
      "source": [
        "# Save the best model\n",
        "best_model.save(f\"***best_model_A3***.keras\")\n",
        "\n",
        "# Check if the model file exists\n",
        "if path.isfile(f\"***best_model_A3***.keras\") == True:\n",
        "    print(\"The best model has been saved successfully.\")\n",
        "else:\n",
        "    print(\"Error Occured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kafb5zs7TnuQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}